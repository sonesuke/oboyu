# Oboyu Benchmark Suite

This directory contains a comprehensive benchmark suite for Oboyu, designed to measure and track performance metrics and accuracy over time.

## Overview

The benchmark suite provides three main evaluation areas:

1. **Speed Benchmarks** (`speed/`): Measure indexing and search performance
2. **Accuracy Benchmarks** (`accuracy/`): Evaluate retrieval accuracy for RAG performance
3. **Reranker Benchmarks** (`reranker/`): Test reranker model effectiveness

## Quick Start

### Unified Benchmark Runner (Recommended)

The easiest way to run benchmarks is using the unified runner:

```bash
# Run all benchmarks quickly (reduced scope)
uv run python bench/run_benchmarks.py all --quick

# Run only speed benchmarks
uv run python bench/run_benchmarks.py speed --datasets small medium

# Run only accuracy evaluation
uv run python bench/run_benchmarks.py accuracy --datasets synthetic

# Run comprehensive evaluation (full scope)
uv run python bench/run_benchmarks.py all --comprehensive

# Run reranker-specific benchmarks
uv run python bench/run_benchmarks.py reranker --models small large
```

### Individual Benchmark Modules

You can also run benchmarks individually for more control:

```bash
# Speed benchmarks
uv run python bench/speed/run_speed_benchmark.py --datasets small medium

# Accuracy evaluation
uv run python bench/accuracy/benchmark_rag_accuracy.py --datasets synthetic miracl-ja

# Reranker evaluation
uv run python bench/reranker/benchmark_reranking.py --models cl-nagoya/ruri-reranker-small

# Analyze results
uv run python bench/speed/analyze.py --latest 5
```

### Unified Runner Options

The `run_benchmarks.py` script provides several useful options:

```bash
# Show help for all options
uv run python bench/run_benchmarks.py --help

# Quick benchmarks (reduced scope for faster execution)
uv run python bench/run_benchmarks.py all --quick

# Comprehensive benchmarks (full scope for thorough evaluation)  
uv run python bench/run_benchmarks.py all --comprehensive

# Verbose output for debugging
uv run python bench/run_benchmarks.py speed --verbose

# Custom output directory
uv run python bench/run_benchmarks.py accuracy --output-dir /custom/path
```

## Directory Structure

```
bench/
â”œâ”€â”€ __init__.py                    # Package initialization with exports
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ run_benchmarks.py              # Unified benchmark runner (main entry point)
â”œâ”€â”€ config.py                      # Shared configuration
â”œâ”€â”€ utils.py                       # Common utilities  
â”œâ”€â”€ logger.py                      # Shared logging utilities
â”œâ”€â”€ core/                          # Shared core functionality
â”‚   â”œâ”€â”€ __init__.py               # Core module exports
â”‚   â”œâ”€â”€ benchmark_base.py         # Base benchmark class
â”‚   â””â”€â”€ metrics.py                # Unified metrics calculation
â”œâ”€â”€ speed/                         # Speed benchmarks
â”‚   â”œâ”€â”€ __init__.py               # Speed module initialization
â”‚   â”œâ”€â”€ run_speed_benchmark.py    # Main speed benchmark runner
â”‚   â”œâ”€â”€ benchmark_indexing.py     # Indexing performance tests
â”‚   â”œâ”€â”€ benchmark_search.py       # Search performance tests
â”‚   â”œâ”€â”€ runner.py                 # Speed benchmark orchestration
â”‚   â”œâ”€â”€ analyze.py                # Results analysis
â”‚   â”œâ”€â”€ reporter.py               # Report generation
â”‚   â”œâ”€â”€ results.py                # Results management
â”‚   â”œâ”€â”€ generate_queries.py       # Query generation
â”‚   â””â”€â”€ generate_test_data.py     # Test data generation
â”œâ”€â”€ accuracy/                      # Accuracy evaluation (renamed from rag_accuracy)
â”‚   â”œâ”€â”€ __init__.py               # Accuracy module exports
â”‚   â”œâ”€â”€ benchmark_rag_accuracy.py # Main accuracy benchmark runner
â”‚   â””â”€â”€ rag_accuracy/             # RAG evaluation implementation
â”‚       â”œâ”€â”€ __init__.py           # RAG accuracy module initialization
â”‚       â”œâ”€â”€ rag_evaluator.py      # Core RAG evaluation logic
â”‚       â”œâ”€â”€ dataset_manager.py    # JMTEB dataset management
â”‚       â”œâ”€â”€ metrics_calculator.py # IR metrics calculation
â”‚       â”œâ”€â”€ reranker_evaluator.py # Reranking evaluation
â”‚       â””â”€â”€ results_analyzer.py   # Results analysis and reporting
â”œâ”€â”€ reranker/                      # Reranker-specific benchmarks (new)
â”‚   â”œâ”€â”€ __init__.py               # Reranker module exports
â”‚   â””â”€â”€ benchmark_reranking.py    # Reranker evaluation benchmarks
â”œâ”€â”€ config/                        # Configuration files
â”‚   â””â”€â”€ rag_eval_config.yaml      # RAG evaluation configuration
â”œâ”€â”€ data/                          # Test datasets (generated)
â”‚   â”œâ”€â”€ small/                    # Small dataset (~50 files)
â”‚   â”œâ”€â”€ medium/                   # Medium dataset (~1,000 files)
â”‚   â””â”€â”€ large/                    # Large dataset (~10,000 files)
â”œâ”€â”€ queries/                       # Query datasets (generated)
â”‚   â”œâ”€â”€ japanese_queries.json
â”‚   â”œâ”€â”€ english_queries.json
â”‚   â””â”€â”€ mixed_queries.json
â””â”€â”€ results/                       # Benchmark results and reports
    â”œâ”€â”€ speed_*.json              # Speed benchmark results
    â”œâ”€â”€ accuracy_*.json           # Accuracy benchmark results  
    â”œâ”€â”€ reranker_*.json           # Reranker benchmark results
    â”œâ”€â”€ *_report_*.txt            # Human-readable reports
    â””â”€â”€ *_summary_*.json          # Summary files for CI/CD
```

## Speed Benchmarks

### Generate Test Datasets

```bash
# Generate all dataset sizes
uv run python bench/speed/generate_test_data.py all

# Generate specific dataset size
uv run python bench/speed/generate_test_data.py small medium

# Custom output directory
uv run python bench/speed/generate_test_data.py small --output-dir /path/to/data
```

Dataset characteristics:
- **Small**: 50 files, 1-5KB each
- **Medium**: 1,000 files, 2-10KB each  
- **Large**: 10,000 files, 3-15KB each

### Generate Query Datasets

```bash
# Generate all query languages
uv run python bench/speed/generate_queries.py

# Generate specific languages
uv run python bench/speed/generate_queries.py --languages japanese english
```

Query characteristics:
- **Japanese**: 50 queries (technical, business, general, code)
- **English**: 50 queries (technical, business, general, code)
- **Mixed**: 20 queries (Japanese-English mixed)

## RAG Accuracy Benchmarks

The RAG accuracy evaluation framework measures retrieval quality using standard Information Retrieval (IR) metrics on Japanese datasets based on JMTEB (Japanese Massive Text Embedding Benchmark).

### Supported Datasets

- **MIRACL-ja**: Academic research papers and queries
- **MLDR-ja**: Long document retrieval scenarios  
- **JaGovFaqs-22k**: Government FAQ-style content
- **JaCWIR**: Casual web information retrieval

### Available Metrics

- **Precision@K**: Proportion of relevant documents in top-K results
- **Recall@K**: Proportion of relevant documents retrieved in top-K  
- **NDCG@K**: Normalized Discounted Cumulative Gain at K
- **MRR**: Mean Reciprocal Rank
- **Hit Rate**: Percentage of queries with at least one relevant result
- **F1@K**: Harmonic mean of precision and recall at K

### Running Accuracy Evaluations

```bash
# Run via unified runner (recommended)
uv run python bench/run_benchmarks.py accuracy --datasets synthetic miracl-ja

# Or run directly
uv run python bench/accuracy/benchmark_rag_accuracy.py

# Evaluate specific datasets
uv run python bench/accuracy/benchmark_rag_accuracy.py --datasets miracl-ja mldr-ja

# Evaluate with specific search modes and top-k values
uv run python bench/accuracy/benchmark_rag_accuracy.py --search-modes vector hybrid --top-k-values 1 5 10

# Generate report from existing results
uv run python bench/accuracy/benchmark_rag_accuracy.py --report-only

# Compare with previous results for regression detection
uv run python bench/accuracy/benchmark_rag_accuracy.py --compare-with bench/results/accuracy_20250101_120000.json
```

## Speed Benchmarks

### Running Speed Benchmarks

#### Command Line Options

```bash
uv run python bench/speed/run_speed_benchmark.py [options]
```

Options:
- `--datasets SIZES`: Specify dataset sizes (small, medium, large)
- `--languages LANGS`: Specify query languages (japanese, english, mixed)
- `--skip-indexing`: Skip indexing benchmarks
- `--skip-search`: Skip search benchmarks
- `--use-existing-indexes`: Use existing test indexes
- `--force-regenerate`: Force regeneration of test data
- `--analyze-only`: Only analyze existing results

#### Examples

```bash
# Run benchmark on medium dataset with Japanese queries only
uv run python bench/speed/run_speed_benchmark.py --datasets medium --languages japanese

# Skip indexing and use existing indexes for search benchmark
uv run python bench/speed/run_speed_benchmark.py --skip-indexing --use-existing-indexes

# Force regenerate all test data and run benchmark
uv run python bench/speed/run_speed_benchmark.py --datasets small medium large --force-regenerate
```

## Understanding Results

### Indexing Metrics

- **Total Time**: Total time to index the dataset
- **File Discovery Time**: Time to find all files
- **Content Extraction Time**: Time to read and process files
- **Embedding Generation Time**: Time to generate embeddings
- **Database Storage Time**: Time to store in database
- **Files/Second**: Throughput metric for files
- **Chunks/Second**: Throughput metric for chunks

### Search Metrics

- **Response Time**: Time to execute a single query
- **Queries/Second (QPS)**: Throughput metric
- **First Result Time**: Time to get first result
- **Statistics**: Mean, median, P95, P99 response times

### System Metrics

- **CPU Usage**: Average and peak CPU usage
- **Memory Usage**: Average and peak memory usage
- **Disk I/O**: Read and write metrics

## Analyzing Performance

### View Latest Results

```bash
# Analyze last 5 runs
uv run python bench/analyze.py --latest 5

# Analyze with custom results directory
uv run python bench/analyze.py --results-dir /path/to/results
```

### Compare Runs

```bash
# Compare two specific runs
uv run python bench/analyze.py --compare <run_id_1> <run_id_2>

# Set regression threshold (default: 10%)
uv run python bench/analyze.py --compare <run_id_1> <run_id_2> --regression-threshold 0.05
```

### Performance Trends

The analyzer automatically detects:
- Performance improvements (ğŸ“ˆ)
- Performance regressions (ğŸ“‰)
- Stable performance (â¡ï¸)

## Output Formats

### JSON Results

Complete benchmark data in `results/benchmark_run_*.json`:
- System information
- Configuration used
- Raw timing data
- Individual query results

### Text Reports

Human-readable reports in `results/report_*.txt`:
- Summary statistics
- Performance breakdown
- System usage metrics

### Summary Files

Quick access summaries in `results/summary_*.json`:
- Key metrics only
- Easy to parse for CI/CD

## Configuration

Edit `config.py` to customize:

```python
# Dataset sizes
DATASET_SIZES = {
    "small": {
        "file_count": 50,
        "content_size_range": (1000, 5000)
    },
    # ...
}

# Benchmark settings
BENCHMARK_CONFIG = {
    "indexing": {
        "warmup_runs": 1,
        "test_runs": 3
    },
    "search": {
        "warmup_runs": 5,
        "test_runs": 100
    }
}
```

## Integration with CI/CD

### Basic Integration

```bash
# Run quick benchmark suite and check for regressions
uv run python bench/run_benchmarks.py all --quick
uv run python bench/speed/analyze.py --latest 2 --regression-threshold 0.1
```

### GitHub Actions Example

```yaml
- name: Run Quick Performance Benchmark
  run: uv run python bench/run_benchmarks.py all --quick
  
- name: Check for Speed Regressions
  run: |
    uv run python bench/speed/analyze.py --latest 2 --regression-threshold 0.1
    if [ $? -ne 0 ]; then
      echo "Performance regression detected!"
      exit 1
    fi

- name: Run Accuracy Benchmark (Weekly)
  if: github.event_name == 'schedule'
  run: uv run python bench/run_benchmarks.py accuracy --comprehensive
```

## Troubleshooting

### Common Issues

1. **"Oboyu is not installed"**
   ```bash
   uv pip install -e .
   ```

2. **"Dataset not found"**
   ```bash
   uv run python bench/speed/generate_test_data.py all
   ```

3. **"Query file not found"**
   ```bash
   uv run python bench/speed/generate_queries.py
   ```

4. **Memory issues with large dataset**
   - Reduce batch size in `config.py`
   - Use smaller dataset for testing

### Debug Mode

Enable verbose output:
```bash
# For unified runner
uv run python bench/run_benchmarks.py speed --datasets small --verbose

# For individual benchmarks
uv run python bench/speed/run_speed_benchmark.py --datasets small --verbose
```

## RAG Accuracy Evaluation

### Overview

The RAG accuracy evaluation suite measures Oboyu's effectiveness as a complete RAG (Retrieval-Augmented Generation) system, with special focus on Japanese document search performance.

### Running RAG Accuracy Benchmarks

**Important**: The RAG accuracy benchmark requires the Ruri v3 embedding model (~90MB) to be downloaded on first run. Ensure you have a stable internet connection.

```bash
# Run via unified runner (recommended)
uv run python bench/run_benchmarks.py accuracy --datasets synthetic

# Run with synthetic dataset (quick test)
uv run python bench/accuracy/benchmark_rag_accuracy.py --datasets synthetic

# Run with Japanese evaluation datasets  
uv run python bench/accuracy/benchmark_rag_accuracy.py --datasets miracl-ja mldr-ja jagovfaqs-22k

# Evaluate specific search modes
uv run python bench/accuracy/benchmark_rag_accuracy.py --search-modes vector hybrid

# Include reranking evaluation
uv run python bench/accuracy/benchmark_rag_accuracy.py --evaluate-reranking
```

### RAG Evaluation Datasets

#### Available Datasets
- **synthetic**: Basic synthetic Japanese dataset for testing
- **miracl-ja**: Multilingual information retrieval (Japanese) - Academic papers and research
- **mldr-ja**: Long document retrieval (Japanese) - Government reports and policy documents
- **jagovfaqs-22k**: Japanese government FAQ dataset - Administrative procedures
- **jacwir**: Japanese casual web information retrieval - Blog posts and web content

Note: These are synthetic implementations that mimic the structure and characteristics of real JMTEB datasets. They provide realistic Japanese content for testing the RAG evaluation framework.

#### Custom Datasets
```bash
# Evaluate with custom dataset
uv run python bench/benchmark_rag_accuracy.py --datasets custom --custom-dataset-path /path/to/dataset.json
```

### RAG Metrics

#### Retrieval Metrics
- **Precision@K**: Fraction of retrieved documents that are relevant
- **Recall@K**: Fraction of relevant documents that are retrieved
- **NDCG@K**: Normalized Discounted Cumulative Gain
- **MRR**: Mean Reciprocal Rank
- **Hit Rate**: Percentage of queries with at least one relevant result

#### Reranking Metrics (Planned)
- **Ranking Improvement**: Improvement over initial retrieval
- **MAP**: Mean Average Precision
- **Position Improvement**: Average position change for relevant documents

### Analysis and Reporting

```bash
# Generate report from existing results
uv run python bench/accuracy/benchmark_rag_accuracy.py --report-only

# Compare with previous run
uv run python bench/accuracy/benchmark_rag_accuracy.py --compare-with bench/results/accuracy_20250101_120000.json

# Set custom regression threshold (default: 10%)
uv run python bench/accuracy/benchmark_rag_accuracy.py --compare-with previous.json --regression-threshold 0.05
```

### Configuration

Edit `bench/config/rag_eval_config.yaml` to customize:
- Dataset selection
- Evaluation metrics
- Performance baselines
- Resource limits
- Output formats

## Future Enhancements

- [x] Accuracy measurement benchmarks (RAG evaluation implemented)
- [ ] BM25 and hybrid search benchmarks (partially implemented in RAG evaluation)
- [ ] Multi-threaded indexing benchmarks
- [ ] Network-based MCP server benchmarks
- [ ] Automated performance regression alerts
- [ ] Grafana dashboard integration
- [ ] Real-time RAG evaluation during development
- [ ] Cross-language RAG evaluation (Japanese-English)

## Contributing

When adding new benchmarks:

1. Follow the existing pattern in `benchmark_*.py`
2. Add configuration to `config.py`
3. Update result classes in `results.py`
4. Add reporting logic to `reporter.py`
5. Document in this README

## License

Same as Oboyu project (MIT License)