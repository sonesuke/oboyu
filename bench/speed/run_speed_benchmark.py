#!/usr/bin/env python3
"""Main entry point for running Oboyu speed benchmarks."""

import argparse
import sys
from pathlib import Path

# Add bench parent directory to path for importing bench modules
bench_parent = Path(__file__).parent.parent
sys.path.insert(0, str(bench_parent.parent))
sys.path.insert(0, str(bench_parent))

# Add src directory to path for importing oboyu modules
src_path = Path(__file__).parent.parent.parent / "src"
if src_path.exists():
    sys.path.insert(0, str(src_path))

from rich.console import Console

from bench.config import DATA_DIR, DATASET_SIZES, QUERIES_DIR, RESULTS_DIR, get_query_languages
from bench.speed.analyze import BenchmarkAnalyzer
from bench.speed.runner import BenchmarkRunner
from bench.utils import check_oboyu_installation, print_header

console = Console()


def main() -> None:
    """Run speed benchmarks."""
    parser = argparse.ArgumentParser(
        description="Run Oboyu performance benchmarks",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run benchmark with small dataset
  python bench/run_speed_benchmark.py --datasets small
  
  # Run benchmark with multiple datasets
  python bench/run_speed_benchmark.py --datasets small medium
  
  # Skip indexing benchmarks
  python bench/run_speed_benchmark.py --datasets small --skip-indexing
  
  # Analyze results after running
  python bench/run_speed_benchmark.py --analyze-only
        """
    )
    
    # Benchmark mode
    parser.add_argument(
        "--analyze-only",
        action="store_true",
        help="Only analyze existing results"
    )
    
    # Dataset selection
    parser.add_argument(
        "--datasets",
        nargs="+",
        choices=list(DATASET_SIZES.keys()),
        help="Dataset sizes to benchmark"
    )
    parser.add_argument(
        "--languages",
        nargs="+",
        choices=get_query_languages(),
        help="Query languages to test"
    )
    
    # Benchmark options
    parser.add_argument(
        "--skip-indexing",
        action="store_true",
        help="Skip indexing benchmarks"
    )
    parser.add_argument(
        "--skip-search",
        action="store_true",
        help="Skip search benchmarks"
    )
    parser.add_argument(
        "--use-existing-indexes",
        action="store_true",
        help="Use existing test indexes instead of creating new ones"
    )
    parser.add_argument(
        "--force-regenerate",
        action="store_true",
        help="Force regeneration of test data and queries"
    )
    
    # Directory options
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=DATA_DIR,
        help="Directory for test datasets"
    )
    parser.add_argument(
        "--queries-dir",
        type=Path,
        default=QUERIES_DIR,
        help="Directory for query datasets"
    )
    parser.add_argument(
        "--results-dir",
        type=Path,
        default=RESULTS_DIR,
        help="Directory for benchmark results"
    )
    
    args = parser.parse_args()
    
    # Check Oboyu installation
    if not check_oboyu_installation():
        console.print("[red]Error: Oboyu is not installed[/red]")
        console.print("Please install Oboyu first: pip install -e .")
        sys.exit(1)
    
    # Analyze only mode
    if args.analyze_only:
        print_header("Benchmark Analysis")
        analyzer = BenchmarkAnalyzer(args.results_dir)
        
        runs_info = analyzer.results_manager.list_runs()
        if not runs_info:
            console.print("[yellow]No benchmark results found[/yellow]")
            sys.exit(0)
        
        # Load and analyze latest runs
        runs = []
        for info in runs_info[:5]:  # Latest 5 runs
            run = analyzer.results_manager.get_run(info["run_id"])
            if run:
                runs.append(run)
        
        runs.reverse()  # Chronological order
        analyzer.print_analysis(runs)
        sys.exit(0)
    
    # Determine datasets and languages
    datasets = args.datasets or ["small"]
    languages = args.languages or ["english", "japanese"]
    
    # Create and run benchmark
    try:
        runner = BenchmarkRunner(
            dataset_sizes=datasets,
            query_languages=languages,
            data_dir=args.data_dir,
            queries_dir=args.queries_dir,
            results_dir=args.results_dir
        )
        
        run = runner.run(
            force_regenerate=args.force_regenerate,
            skip_indexing=args.skip_indexing,
            skip_search=args.skip_search,
            use_existing_indexes=args.use_existing_indexes
        )
        
        console.print("\n[green]âœ¨ Benchmark completed successfully![/green]")
        console.print(f"Run ID: {run.run_id}")
        
        # Offer to analyze results
        console.print("\nTo analyze results, run:")
        console.print("  python bench/analyze.py --latest 5")
        console.print(f"  python bench/analyze.py --compare {run.run_id} <previous_run_id>")
        
    except KeyboardInterrupt:
        console.print("\n[yellow]Benchmark interrupted by user[/yellow]")
        sys.exit(1)
    except Exception as e:
        console.print(f"\n[red]Error: {e}[/red]")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
