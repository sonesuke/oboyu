"""Utilities for handling Hugging Face Hub connectivity and errors."""

from __future__ import annotations

import logging
import socket
import time
from collections.abc import Callable
from typing import TYPE_CHECKING, TypeVar

import httpx
from huggingface_hub import HfApi
from huggingface_hub.errors import (
    HfHubHTTPError,
    RepositoryNotFoundError,
)

# Import HuggingFace utils with fallback for older versions
EntryNotFoundError: type[Exception]
LocalEntryNotFoundError: type[Exception]
OfflineModeIsEnabled: type[Exception]

try:
    from huggingface_hub.utils import (
        EntryNotFoundError,
        LocalEntryNotFoundError,
        OfflineModeIsEnabled,
    )
except ImportError:
    # Fallback for older versions of huggingface_hub
    class EntryNotFoundError(Exception):  # type: ignore[misc,no-redef]
        """Fallback exception for entry not found errors."""

        pass
    
    class LocalEntryNotFoundError(Exception):  # type: ignore[misc,no-redef]
        """Fallback exception for local entry not found errors."""

        pass
    
    class OfflineModeIsEnabled(Exception):  # type: ignore[misc,no-redef]
        """Fallback exception for offline mode errors."""

        pass

if TYPE_CHECKING:
    from pathlib import Path

logger = logging.getLogger(__name__)

T = TypeVar("T")


class HuggingFaceError(Exception):
    """Base exception for Hugging Face related errors."""

    def __init__(self, message: str, technical_details: str | None = None) -> None:
        """Initialize the error with a user-friendly message and optional technical details."""
        super().__init__(message)
        self.message = message
        self.technical_details = technical_details


class HuggingFaceNetworkError(HuggingFaceError):
    """Raised when network connectivity issues prevent accessing Hugging Face Hub."""


class HuggingFaceModelNotFoundError(HuggingFaceError):
    """Raised when a requested model cannot be found on Hugging Face Hub."""


class HuggingFaceRateLimitError(HuggingFaceError):
    """Raised when Hugging Face API rate limits are exceeded."""


class HuggingFaceAuthenticationError(HuggingFaceError):
    """Raised when authentication is required to access a model."""


class HuggingFaceTimeoutError(HuggingFaceError):
    """Raised when operations timeout while accessing Hugging Face Hub."""


def check_huggingface_connectivity(timeout: float = 5.0) -> bool:
    """Check if Hugging Face Hub is accessible.

    Args:
        timeout: Maximum time to wait for response in seconds.

    Returns:
        True if Hub is accessible, False otherwise.

    """
    try:
        with httpx.Client() as client:
            response = client.get(
                "https://huggingface.co/api/models",
                timeout=timeout,
                params={"limit": 1},
            )
            return response.status_code == 200
    except (httpx.ConnectError, httpx.TimeoutException, socket.gaierror):
        return False
    except Exception:
        logger.exception("Unexpected error checking Hugging Face connectivity")
        return False


def safe_model_download(
    model_id: str,
    download_func: Callable[[], T],
    max_retries: int = 3,
    initial_delay: float = 1.0,
    backoff_factor: float = 2.0,
    cache_dir: Path | None = None,
) -> T:
    """Safely download a model with retry logic and error handling.

    Args:
        model_id: The model identifier (e.g., 'cl-nagoya/ruri-v3-30m').
        download_func: Function that performs the actual download.
        max_retries: Maximum number of retry attempts.
        initial_delay: Initial delay between retries in seconds.
        backoff_factor: Factor by which to multiply delay after each retry.
        cache_dir: Directory where models are cached.

    Returns:
        The result of the download function.

    Raises:
        HuggingFaceError: Various subclasses depending on the error type.

    """
    last_error: Exception | None = None
    delay = initial_delay

    for attempt in range(max_retries + 1):
        try:
            if attempt > 0:
                logger.info(f"Retry attempt {attempt}/{max_retries} for model {model_id}")
                time.sleep(delay)
                delay *= backoff_factor

            return download_func()

        except OfflineModeIsEnabled:
            # Check if model is available in cache
            if cache_dir and _is_model_cached(model_id, cache_dir):
                logger.info(f"Using cached model {model_id} in offline mode")
                return download_func()
            raise HuggingFaceNetworkError(
                message="Cannot download model in offline mode",
                technical_details="Hugging Face Hub is set to offline mode and model is not cached",
            )

        except RepositoryNotFoundError as e:
            raise HuggingFaceModelNotFoundError(
                message=f"Model '{model_id}' not found on Hugging Face Hub",
                technical_details=str(e),
            ) from e

        except HfHubHTTPError as e:
            if e.response.status_code == 401:
                raise HuggingFaceAuthenticationError(
                    message=f"Authentication required to access model '{model_id}'",
                    technical_details="This model may be private. Please set HF_TOKEN environment variable.",
                ) from e
            elif e.response.status_code == 429:
                if attempt < max_retries:
                    # Extract retry-after header if available
                    retry_after = e.response.headers.get("retry-after")
                    if retry_after:
                        delay = float(retry_after)
                    logger.warning(f"Rate limit hit, waiting {delay} seconds")
                    last_error = e
                    continue
                raise HuggingFaceRateLimitError(
                    message="Hugging Face API rate limit exceeded",
                    technical_details="Please wait 15-30 minutes before retrying or use an access token for higher limits.",
                ) from e
            else:
                last_error = e

        except (httpx.ConnectError, httpx.TimeoutException, socket.gaierror) as e:
            if not check_huggingface_connectivity():
                raise HuggingFaceNetworkError(
                    message="Cannot connect to Hugging Face Hub",
                    technical_details="Please check your internet connection and firewall settings.",
                ) from e
            last_error = e

        except (EntryNotFoundError, LocalEntryNotFoundError) as e:
            # These can occur when a specific file within a model repo is not found
            raise HuggingFaceModelNotFoundError(
                message=f"Required files not found for model '{model_id}'",
                technical_details=str(e),
            ) from e

        except Exception as e:
            logger.exception(f"Unexpected error downloading model {model_id}")
            last_error = e

    # If we exhausted all retries
    if isinstance(last_error, (httpx.TimeoutException, TimeoutError)):
        raise HuggingFaceTimeoutError(
            message=f"Timeout downloading model '{model_id}'",
            technical_details="The download is taking too long. This may be due to slow connection or large model size.",
        ) from last_error
    else:
        raise HuggingFaceNetworkError(
            message=f"Failed to download model '{model_id}' after {max_retries} retries",
            technical_details=str(last_error) if last_error else None,
        ) from last_error


def get_user_friendly_error_message(error: HuggingFaceError) -> str:
    """Convert a HuggingFaceError to a user-friendly message with guidance.

    Args:
        error: The error to convert.

    Returns:
        A formatted string with clear guidance for the user.

    """
    if isinstance(error, HuggingFaceNetworkError):
        return (
            "‚ùå Cannot connect to Hugging Face Hub\n\n"
            "Please check:\n"
            "‚Ä¢ Internet connection is working\n"
            "‚Ä¢ Firewall is not blocking huggingface.co\n"
            "‚Ä¢ VPN settings (try disabling temporarily)\n"
            "‚Ä¢ Try again in a few minutes\n\n"
            "The system will attempt to use cached models if available."
        )

    elif isinstance(error, HuggingFaceModelNotFoundError):
        model_id = _extract_model_id_from_error(error)
        return (
            f"‚ùå Model '{model_id}' not found\n\n"
            "Please check:\n"
            f"‚Ä¢ Model name spelling is correct\n"
            f"‚Ä¢ Model exists at: https://huggingface.co/{model_id}\n"
            "‚Ä¢ Organization and repository names are correct\n\n"
            "Recommended Japanese models:\n"
            "‚Ä¢ cl-nagoya/ruri-v3-30m (embeddings)\n"
            "‚Ä¢ cl-nagoya/ruri-reranker-small (reranking)"
        )

    elif isinstance(error, HuggingFaceRateLimitError):
        return (
            "‚è∞ Hugging Face rate limit reached\n\n"
            "Please wait 15-30 minutes before retrying.\n"
            "Consider using a Hugging Face access token for higher limits:\n"
            "‚Ä¢ Create token at: https://huggingface.co/settings/tokens\n"
            "‚Ä¢ Set environment variable: export HF_TOKEN=your_token"
        )

    elif isinstance(error, HuggingFaceAuthenticationError):
        return (
            "üîí Access denied to model\n\n"
            "This model may be private and require authentication.\n"
            "Please set up a Hugging Face access token:\n"
            "‚Ä¢ Create token at: https://huggingface.co/settings/tokens\n"
            "‚Ä¢ Set environment variable: export HF_TOKEN=your_token\n"
            "‚Ä¢ Ensure you have access to the private model"
        )

    elif isinstance(error, HuggingFaceTimeoutError):
        return (
            "‚è±Ô∏è Download timeout\n\n"
            "The model download is taking too long. This could be due to:\n"
            "‚Ä¢ Slow internet connection\n"
            "‚Ä¢ Large model size\n"
            "‚Ä¢ Temporary server issues\n\n"
            "Please try:\n"
            "‚Ä¢ Check your internet speed\n"
            "‚Ä¢ Try again during off-peak hours\n"
            "‚Ä¢ Use a smaller model variant if available"
        )

    else:
        return (
            "‚ùå Hugging Face Hub error\n\n"
            f"Error: {error.message}\n\n"
            "Please try again or check the logs for more details."
        )


def validate_model_exists(model_id: str) -> bool:
    """Check if a model exists on Hugging Face Hub.

    Args:
        model_id: The model identifier to check.

    Returns:
        True if the model exists, False otherwise.

    """
    try:
        api = HfApi()
        info = api.model_info(model_id)
        return info is not None
    except Exception:
        return False


def get_model_suggestions(partial_name: str, task: str | None = None) -> list[str]:
    """Get model suggestions based on partial name and optional task.

    Args:
        partial_name: Partial model name to search for.
        task: Optional task filter (e.g., 'sentence-similarity', 'text-classification').

    Returns:
        List of suggested model IDs.

    """
    try:
        api = HfApi()
        models = api.list_models(
            search=partial_name,
            task=task,
            limit=5,
            sort="downloads",
            direction=-1,
        )
        return [model.id for model in models]
    except Exception:
        logger.exception("Failed to get model suggestions")
        return []


def _is_model_cached(model_id: str, cache_dir: Path) -> bool:
    """Check if a model is available in the local cache.

    Args:
        model_id: The model identifier.
        cache_dir: The cache directory to check.

    Returns:
        True if the model appears to be cached, False otherwise.

    """
    # This is a simplified check - actual caching structure may vary
    model_path = cache_dir / "models--" / model_id.replace("/", "--")
    return model_path.exists() and any(model_path.iterdir())


def _extract_model_id_from_error(error: HuggingFaceError) -> str:
    """Extract model ID from error message if possible.

    Args:
        error: The error to extract from.

    Returns:
        The model ID or 'unknown' if not found.

    """
    import re

    # Try to extract model ID from error message
    if match := re.search(r"'([^']+/[^']+)'", error.message):
        return match.group(1)
    return "unknown"


def get_fallback_models(model_type: str) -> list[tuple[str, str]]:
    """Get fallback model suggestions based on model type.

    Args:
        model_type: Type of model ('embedding', 'reranker', 'tokenizer').

    Returns:
        List of (model_id, description) tuples.

    """
    fallbacks = {
        "embedding": [
            ("cl-nagoya/ruri-v3-30m", "Japanese text embeddings (30M parameters)"),
            ("cl-nagoya/ruri-v3-base", "Japanese text embeddings (base size)"),
            ("sentence-transformers/all-MiniLM-L6-v2", "Multilingual embeddings"),
        ],
        "reranker": [
            ("cl-nagoya/ruri-reranker-small", "Japanese text reranking (small)"),
            ("cl-nagoya/ruri-reranker-base", "Japanese text reranking (base)"),
            ("BAAI/bge-reranker-base", "Multilingual reranking"),
        ],
        "tokenizer": [
            ("cl-tohoku/bert-base-japanese-v3", "Japanese BERT tokenizer"),
            ("line-corporation/line-distilbert-base-japanese", "Japanese DistilBERT"),
            ("bert-base-multilingual-cased", "Multilingual BERT tokenizer"),
        ],
    }
    return fallbacks.get(model_type, [])

