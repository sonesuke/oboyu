---
id: japanese
title: Japanese Language Support in Oboyu
sidebar_position: 90
---

# Japanese Language Support in Oboyu

Oboyu provides world-class support for Japanese text processing with advanced features designed specifically for the unique characteristics of the Japanese language. This includes automatic encoding detection, intelligent text normalization, morphological analysis, and optimized search algorithms that understand Japanese grammar and writing systems.

## Table of Contents

- [Core Features](#core-features)
- [Encoding Detection](#encoding-detection)
- [Text Normalization](#text-normalization)
- [Tokenization and Morphological Analysis](#tokenization-and-morphological-analysis)
- [Search Optimization](#search-optimization)
- [Mixed Language Support](#mixed-language-support)
- [Performance Benchmarks](#performance-benchmarks)
- [Best Practices](#best-practices)
- [Troubleshooting](#troubleshooting)
- [Advanced Configuration](#advanced-configuration)

## Core Features

Oboyu's Japanese support includes:

- ðŸ” **Automatic Encoding Detection**: Handles UTF-8, Shift-JIS, EUC-JP, CP932, and ISO-2022-JP
- ðŸ”¤ **Intelligent Normalization**: Full/half-width conversion, variant unification
- ðŸ”¬ **Morphological Analysis**: MeCab and SentencePiece integration
- ðŸŽ¯ **Optimized Models**: Japanese-specific embedding models (Ruri v3)
- ðŸŒ **Mixed Language**: Seamless Japanese-English document handling
- âš¡ **High Performance**: Optimized for Japanese text characteristics

## Encoding Detection

### Overview

Japanese text files often use various character encodings due to historical reasons. Oboyu automatically detects and handles multiple Japanese encodings to ensure all documents are properly indexed.

### Supported Encodings

Oboyu supports automatic detection and conversion of common Japanese encodings:

- **UTF-8**: Modern standard encoding (default)
- **Shift-JIS**: Windows Japanese encoding (CP932)
- **EUC-JP**: Unix/Linux Japanese encoding
- **ISO-2022-JP**: Email and older systems encoding

### How It Works

When processing files, Oboyu:

1. **Attempts UTF-8 first** as the modern standard
2. **Detects encoding** using byte patterns specific to Japanese encodings
3. **Converts to UTF-8** internally for consistent processing
4. **Handles mixed encodings** in different files within the same directory

### Automatic Detection

Oboyu automatically detects and handles Japanese encodings without requiring configuration. The system intelligently identifies the encoding of each file and converts it to UTF-8 for consistent processing.

## Text Normalization

### Unicode Normalization

All Japanese text undergoes Unicode NFKC (Normalization Form KC) normalization to ensure consistent matching:

```
Input:  "ï½ˆï½…ï½Œï½Œï½ã€€ãƒ¯ãƒ¼ãƒ«ãƒ‰"  (mixed width)
Output: "hello ãƒ¯ãƒ¼ãƒ«ãƒ‰"      (normalized)
```

This normalization handles:
- **Full-width to half-width conversion** for ASCII characters
- **Character variant unification** (ç•°ä½“å­—ã®çµ±ä¸€)
- **Combining character normalization**

### Character Width Normalization

Oboyu normalizes character widths for consistent search:

```
Full-width: ï¼¡ï¼¢ï¼£ï¼‘ï¼’ï¼“
Half-width: ABC123

Full-width: ï½¶ï¾€ï½¶ï¾…
Full-width: ã‚«ã‚¿ã‚«ãƒŠ
```

### Benefits

- **Consistent search results** regardless of input method
- **Improved matching** between different document sources
- **Reduced index size** through normalization

## Japanese Text Processing Pipeline

### Processing Steps

1. **File Reading**
   - Detect encoding using configured encoding list
   - Read file with detected encoding

2. **Text Extraction**
   - Convert to UTF-8 if needed
   - Extract text content

3. **Normalization**
   - Apply Unicode NFKC normalization
   - Normalize character widths
   - Clean up whitespace

4. **Tokenization** (for BM25 search)
   - Use MeCab for morphological analysis
   - Extract meaningful tokens

### Example Pipeline

```python
# Original file (Shift-JIS encoded)
"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€€è¨­è¨ˆã€€ãƒ‘ã‚¿ãƒ¼ãƒ³"

# After encoding detection and conversion
"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€€è¨­è¨ˆã€€ãƒ‘ã‚¿ãƒ¼ãƒ³" (UTF-8)

# After normalization
"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ è¨­è¨ˆ ãƒ‘ã‚¿ãƒ¼ãƒ³" (normalized spaces)

# After tokenization
["ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "è¨­è¨ˆ", "ãƒ‘ã‚¿ãƒ¼ãƒ³"]
```

## Tokenization and Morphological Analysis

### MeCab Integration

Oboyu integrates with MeCab for accurate Japanese morphological analysis:

```bash
# Install MeCab (optional but recommended)
# macOS
brew install mecab mecab-ipadic

# Ubuntu/Debian
sudo apt-get install mecab libmecab-dev mecab-ipadic-utf8

# Install Python binding
pip install mecab-python3
```

### Tokenization Examples

#### Basic Tokenization
```python
# Input text
"è‡ªç„¶è¨€èªžå‡¦ç†ã‚’å­¦ç¿’ã™ã‚‹"

# MeCab tokenization
["è‡ªç„¶", "è¨€èªž", "å‡¦ç†", "ã‚’", "å­¦ç¿’", "ã™ã‚‹"]

# With part-of-speech filtering (nouns and verbs only)
["è‡ªç„¶", "è¨€èªž", "å‡¦ç†", "å­¦ç¿’"]
```

#### Complex Sentences
```python
# Input
"æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®æœ€é©åŒ–æ‰‹æ³•ã«ã¤ã„ã¦èª¬æ˜Žã—ã¾ã™ã€‚"

# Tokenized output
["æ©Ÿæ¢°", "å­¦ç¿’", "ãƒ¢ãƒ‡ãƒ«", "æ€§èƒ½", "å‘ä¸Š", "æœ€é©", "åŒ–", "æ‰‹æ³•", "èª¬æ˜Ž"]
```

#### Named Entity Recognition
```python
# Company names and products
"æ ªå¼ä¼šç¤¾OpenAIã®GPT-4ã¯é©æ–°çš„ãªAIãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚"

# Intelligent tokenization preserves entities
["æ ªå¼ä¼šç¤¾", "OpenAI", "GPT-4", "é©æ–°", "çš„", "AI", "ãƒ¢ãƒ‡ãƒ«"]
```

### SentencePiece Fallback

When MeCab is not available, Oboyu uses SentencePiece:

```python
# SentencePiece subword tokenization
"æœªçŸ¥ã®å°‚é–€ç”¨èªžã‚‚é©åˆ‡ã«å‡¦ç†"
["â–æœªçŸ¥", "ã®", "â–å°‚é–€", "ç”¨èªž", "ã‚‚", "â–é©åˆ‡", "ã«", "â–å‡¦ç†"]
```

## Search Optimization

### Japanese-Specific Ranking

Oboyu optimizes search ranking for Japanese characteristics:

#### 1. Compound Word Handling
```python
# Query: "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹"
# Matches both:
- "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹" (exact match)
- "ãƒ‡ãƒ¼ã‚¿" + "ãƒ™ãƒ¼ã‚¹" (compound components)
```

#### 2. Reading Variations
```python
# Handles different readings
- "ä¼šè­°" (kaigi)
- "æ‰“ã¡åˆã‚ã›" (uchiawase)
- "ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°" (meeting)
# All treated as related concepts
```

#### 3. Script Mixing
```python
# Query: "AIæŠ€è¡“"
# Intelligently matches:
- "AIæŠ€è¡“"
- "äººå·¥çŸ¥èƒ½æŠ€è¡“"
- "ã‚¨ãƒ¼ã‚¢ã‚¤æŠ€è¡“"
```

### Search Examples

#### Technical Documentation Search
```bash
# Search for machine learning concepts
oboyu query "æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ "

# Results ranked by:
# 1. Exact phrase matches
# 2. All terms present
# 3. Partial term matches
# 4. Semantic similarity
```

#### Business Document Search
```bash
# Search for meeting notes
oboyu query "å–¶æ¥­ä¼šè­° è­°äº‹éŒ² 2024å¹´"

# Intelligent date parsing and entity recognition
```

#### Code Documentation
```bash
# Mixed language search
oboyu query "async/await éžåŒæœŸå‡¦ç†"

# Handles technical terms in both languages
```

## Mixed Language Support

### Seamless Language Detection

Oboyu automatically detects and handles mixed language content:

```python
# Document with mixed content
"""
APIè¨­è¨ˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

1. RESTful Design Principles
   - ãƒªã‚½ãƒ¼ã‚¹æŒ‡å‘ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
   - HTTPãƒ¡ã‚½ãƒƒãƒ‰ã®é©åˆ‡ãªä½¿ç”¨

2. Error Handling
   - ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ã®çµ±ä¸€
   - æ—¥æœ¬èªžã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æä¾›
"""
```

### Language-Aware Chunking

```python
# Intelligent chunk boundaries
- Respects sentence boundaries in both languages
- Maintains context across language switches
- Preserves code blocks and technical terms
```

### Search Across Languages

```bash
# Japanese query finding English content
oboyu query "èªè¨¼ã‚·ã‚¹ãƒ†ãƒ "
# Also finds: "authentication system", "auth module"

# English query finding Japanese content  
oboyu query "database optimization"
# Also finds: "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–", "DBé«˜é€ŸåŒ–"
```

## Performance Benchmarks

### Encoding Detection Speed

| File Size | UTF-8 | Shift-JIS | EUC-JP | Mixed |
|-----------|-------|-----------|---------|--------|
| 1KB | 0.1ms | 0.3ms | 0.3ms | 0.5ms |
| 100KB | 0.5ms | 1.2ms | 1.1ms | 2.1ms |
| 1MB | 2.1ms | 4.8ms | 4.5ms | 8.2ms |
| 10MB | 18ms | 42ms | 39ms | 71ms |

### Tokenization Performance

| Text Length | MeCab | SentencePiece | Hybrid |
|-------------|--------|---------------|--------|
| 100 chars | 0.8ms | 1.2ms | 0.9ms |
| 1,000 chars | 3.2ms | 4.8ms | 3.5ms |
| 10,000 chars | 28ms | 41ms | 30ms |

### Search Performance (10,000 documents)

| Query Type | BM25 | Vector | Hybrid |
|------------|------|---------|--------|
| Single term | 12ms | 45ms | 52ms |
| Phrase | 18ms | 46ms | 58ms |
| Complex | 25ms | 48ms | 65ms |

## Best Practices

### Document Preparation

1. **Consistent Encoding**
   ```bash
   # Convert legacy files to UTF-8
   iconv -f SHIFT-JIS -t UTF-8 old_file.txt > new_file.txt
   ```

2. **Structured Content**
   ```markdown
   # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä»•æ§˜æ›¸
   
   ## æ¦‚è¦
   æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯...
   
   ## Technical Requirements
   - Python 3.10+
   - PostgreSQL 14+
   ```

3. **Metadata Usage**
   ```yaml
   ---
   title: ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆæ›¸
   date: 2024-01-15
   tags: [è¨­è¨ˆ, ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£, API]
   ---
   ```

### Query Optimization

1. **Use Natural Phrases**
   ```bash
   # Good: Natural Japanese
   oboyu query "ãƒ¦ãƒ¼ã‚¶ãƒ¼èªè¨¼ã®å®Ÿè£…æ–¹æ³•"
   
   # Less optimal: Keyword list
   oboyu query "ãƒ¦ãƒ¼ã‚¶ãƒ¼ èªè¨¼ å®Ÿè£… æ–¹æ³•"
   ```

2. **Leverage Filters**
   ```bash
   # Filter by file type for technical docs
   oboyu query "APIè¨­è¨ˆ" --filter "*.md"
   
   # Filter by date for recent content
   oboyu query "ä¼šè­°éŒ²" --filter "2024-*"
   ```

3. **Mode Selection**
   ```bash
   # Semantic for concepts
   oboyu query "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘ã®åŽŸå‰‡" --mode semantic
   
   # Keyword for exact terms
   oboyu query "ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ E001" --mode keyword
   ```

## Best Practices

### For Modern Projects

Oboyu automatically handles all encoding scenarios:

```bash
# Modern UTF-8 documents
oboyu index ./modern_docs

# Legacy systems with Shift-JIS or EUC-JP
oboyu index ./legacy_system

# Mixed encoding environments
oboyu index ./mixed_docs
```

The system will automatically detect and handle UTF-8, Shift-JIS, EUC-JP, CP932, and ISO-2022-JP encodings.

## Troubleshooting

### Encoding Detection Issues

**Symptoms:**
- Garbled characters (æ–‡å­—åŒ–ã‘)
- Missing Japanese text
- Indexing errors

**Solutions:**
1. Check file encoding: `file -i filename.txt`
2. Ensure the file is in a supported encoding (UTF-8, Shift-JIS, EUC-JP, CP932, or ISO-2022-JP)
3. Convert unsupported encodings to UTF-8 before indexing

### Normalization Issues

**Symptoms:**
- Search misses documents with different character widths
- Inconsistent search results

**Solutions:**
1. Ensure all documents are re-indexed after configuration changes
2. Use consistent input methods for queries
3. Check normalization in debug mode

### Debug Mode

Enable debug logging for Japanese processing:

```bash
# Index with debug output
oboyu index ./docs --debug
```

## Performance Considerations

### Encoding Detection Overhead

- **Minimal impact**: ~1-5ms per file
- **Cached results**: Encoding detected once per file
- **Parallel processing**: Multiple files processed concurrently

### Normalization Performance

- **Fast processing**: &lt;1ms per document
- **In-memory operation**: No disk I/O required
- **Single-pass**: Normalization done during initial processing

## Advanced Configuration

### Tokenizer Selection

```yaml
# ~/.config/oboyu/config.yaml
indexer:
  language:
    japanese_tokenizer: "mecab"  # or "sentencepiece"
    compound_splitting: true      # Split compound words
    reading_normalization: true   # Normalize readings
    
crawler:
  encoding_detection:
    enabled: true  # Default: true
    confidence_threshold: 0.8
    fallback_encoding: "utf-8"
    # Custom encoding priority (optional)
    encoding_priority:
      - "utf-8"
      - "shift_jis"
      - "euc_jp"
      - "cp932"
      - "iso2022_jp"
```

### Search Optimization

```yaml
query_engine:
  japanese:
    # Boost exact matches in Japanese
    exact_match_boost: 2.0
    # Enable reading variation matching
    reading_variations: true
    # Compound word handling
    compound_word_search: true
    
  # Japanese-specific reranker
  reranker:
    model: "hotchpotch/japanese-reranker-cross-encoder-small-v1"
    enabled: true
```

### Performance Tuning

```yaml
indexer:
  processing:
    # Larger chunks for Japanese (due to character density)
    chunk_size: 800  # characters, not tokens
    chunk_overlap: 200
    
  # Parallel processing
  japanese_processing:
    max_workers: 4
    batch_size: 100
```

## Integration with Search

The encoding detection and normalization ensure that:

- **Vector search** works with properly encoded text
- **BM25 search** matches normalized tokens correctly
- **Hybrid search** combines both effectively

All search modes benefit from consistent text processing, ensuring reliable results regardless of the original file encoding or character variations.

## Common Issues and Solutions

### Issue: Poor Search Results for Japanese Queries

**Symptoms:**
- English results for Japanese queries
- Missing obvious matches
- Irrelevant results

**Solutions:**

1. **Check tokenization**:
   ```bash
   # Verify MeCab is installed
   which mecab
   
   # Test tokenization
   echo "ãƒ†ã‚¹ãƒˆæ–‡ç« " | mecab
   ```

2. **Use appropriate search mode**:
   ```bash
   # For technical terms
   oboyu query "æ©Ÿæ¢°å­¦ç¿’" --mode hybrid
   
   # For concepts
   oboyu query "äººå·¥çŸ¥èƒ½ã®å¿œç”¨" --mode semantic
   ```

3. **Rebuild index**:
   ```bash
   # Clear and rebuild with debug info
   oboyu index --clear
   oboyu index /path/to/docs --debug
   ```

### Issue: Encoding Problems

**Symptoms:**
- æ–‡å­—åŒ–ã‘ (mojibake/garbled text)
- Question marks or squares
- Missing content

**Solutions:**

1. **Check file encoding**:
   ```bash
   # Detect file encoding
   file -i problematic_file.txt
   
   # Or use nkf
   nkf -g problematic_file.txt
   ```

2. **Convert to UTF-8**:
   ```bash
   # Using iconv
   iconv -f SHIFT-JIS -t UTF-8 input.txt > output.txt
   
   # Using nkf
   nkf -w --overwrite *.txt
   ```

3. **Batch conversion**:
   ```bash
   # Convert all Shift-JIS files in directory
   find . -name "*.txt" -exec sh -c '
     encoding=$(nkf -g "$1")
     if [ "$encoding" = "Shift_JIS" ]; then
       nkf -w --overwrite "$1"
       echo "Converted: $1"
     fi
   ' _ {} \;
   ```

## Tips for Optimal Japanese Search

### 1. Document Structure

- Use clear headings in Japanese
- Include ruby annotations for difficult kanji
- Add English translations for key terms
- Use consistent terminology

### 2. Query Formulation

- Use full sentences for semantic search
- Include context in queries
- Try both kanji and hiragana versions
- Use synonyms and related terms

### 3. Index Optimization

- Separate Japanese and English content when possible
- Use metadata for categorization
- Regular index maintenance
- Monitor search performance

## Future Enhancements

Planned improvements for Japanese support:

1. **Advanced NLP Features**
   - Named entity recognition (NER)
   - Dependency parsing
   - Sentiment analysis

2. **Dictionary Integration**
   - Custom domain dictionaries
   - Synonym expansion
   - Technical term databases

3. **Enhanced Models**
   - Larger Japanese embedding models
   - Domain-specific fine-tuning
   - Multilingual model options
